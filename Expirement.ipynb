{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_examples(word_pairs, k=3):\n",
    "    \"\"\"\n",
    "    Formats data for in-context learning:\n",
    "    Input: \"sing1 -> plur1; sing2 -> plur2; ...; singN -> \"\n",
    "    Target: \"plurN\"\n",
    "    \"\"\"\n",
    "    context = []\n",
    "    for _ in range(k):\n",
    "        s, p = random.choice(word_pairs)\n",
    "        context.append(f\"{s} -> {p}\")\n",
    "    \n",
    "    test_singular, test_plural = random.choice(word_pairs)\n",
    "    context.append(f\"{test_singular} -> \")\n",
    "    \n",
    "    return \" ; \".join(context), test_plural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(dataset, batch_size):\n",
    "    \"\"\"Groups dataset items into batches of specified size\"\"\"\n",
    "    batch = []\n",
    "    for item in dataset:\n",
    "        batch.append(item)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:  # Yield remaining items\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InContextTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, nhead=2, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 512, d_model))\n",
    "        self.layers = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, 128),\n",
    "            num_layers\n",
    "        )\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) + self.pos_embed[:, :x.size(1)]\n",
    "        x = self.layers(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cycle(model, dataset, epochs=10, batch_size=32):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(dataset)\n",
    "        total_loss = 0\n",
    "        for batch in make_batches(dataset, batch_size):\n",
    "            inputs = torch.stack([ex[0] for ex in batch])\n",
    "            targets = torch.stack([ex[1] for ex in batch])\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    # Regular plurals\n",
    "    (\"cat\", \"cats\"),\n",
    "    (\"dog\", \"dogs\"),\n",
    "    (\"car\", \"cars\"),\n",
    "    (\"apple\", \"apples\"),\n",
    "    (\"book\", \"books\"),\n",
    "    (\"tree\", \"trees\"),\n",
    "    (\"cup\", \"cups\"),\n",
    "    (\"pen\", \"pens\"),\n",
    "    (\"chair\", \"chairs\"),\n",
    "    (\"table\", \"tables\"),\n",
    "    (\"house\", \"houses\"),\n",
    "    (\"phone\", \"phones\"),\n",
    "    (\"shoe\", \"shoes\"),\n",
    "    (\"bag\", \"bags\"),\n",
    "    (\"door\", \"doors\"),\n",
    "    (\"window\", \"windows\"),\n",
    "    (\"computer\", \"computers\"),\n",
    "    (\"student\", \"students\"),\n",
    "    (\"teacher\", \"teachers\"),\n",
    "    (\"doctor\", \"doctors\"),\n",
    "    (\"friend\", \"friends\"),\n",
    "    (\"child\", \"children\"),\n",
    "    (\"person\", \"people\"),\n",
    "    (\"man\", \"men\"),\n",
    "    (\"woman\", \"women\"),\n",
    "    (\"mouse\", \"mice\"),\n",
    "    (\"goose\", \"geese\"),\n",
    "    (\"tooth\", \"teeth\"),\n",
    "    (\"foot\", \"feet\"),\n",
    "    (\"fish\", \"fish\"),\n",
    "    (\"sheep\", \"sheep\"),\n",
    "    (\"deer\", \"deer\"),\n",
    "    (\"cactus\", \"cacti\"),\n",
    "    (\"focus\", \"foci\"),\n",
    "    (\"fungus\", \"fungi\"),\n",
    "    (\"nucleus\", \"nuclei\"),\n",
    "    (\"syllabus\", \"syllabi\"),\n",
    "    (\"analysis\", \"analyses\"),\n",
    "    (\"crisis\", \"crises\"),\n",
    "    (\"thesis\", \"theses\"),\n",
    "    (\"phenomenon\", \"phenomena\"),\n",
    "    (\"criterion\", \"criteria\"),\n",
    "    (\"datum\", \"data\"),\n",
    "    (\"bus\", \"buses\"),\n",
    "    (\"box\", \"boxes\"),\n",
    "    (\"fox\", \"foxes\"),\n",
    "    (\"watch\", \"watches\"),\n",
    "    (\"wish\", \"wishes\"),\n",
    "    (\"dish\", \"dishes\"),\n",
    "    (\"baby\", \"babies\"),\n",
    "    (\"city\", \"cities\"),\n",
    "    (\"party\", \"parties\"),\n",
    "    (\"story\", \"stories\"),\n",
    "    (\"berry\", \"berries\"),\n",
    "    (\"family\", \"families\"),\n",
    "    (\"country\", \"countries\"),\n",
    "    (\"lady\", \"ladies\"),\n",
    "    (\"boy\", \"boys\"),\n",
    "    (\"toy\", \"toys\"),\n",
    "    (\"key\", \"keys\"),\n",
    "    (\"day\", \"days\"),\n",
    "    (\"monkey\", \"monkeys\"),\n",
    "    (\"leaf\", \"leaves\"),\n",
    "    (\"wolf\", \"wolves\"),\n",
    "    (\"knife\", \"knives\"),\n",
    "    (\"life\", \"lives\"),\n",
    "    (\"wife\", \"wives\"),\n",
    "    (\"calf\", \"calves\"),\n",
    "    (\"half\", \"halves\"),\n",
    "    (\"loaf\", \"loaves\"),\n",
    "    (\"scarf\", \"scarves\"),\n",
    "    (\"chief\", \"chiefs\"),\n",
    "    (\"roof\", \"roofs\"),\n",
    "    (\"belief\", \"beliefs\"),\n",
    "    (\"chef\", \"chefs\"),\n",
    "    (\"photo\", \"photos\"),\n",
    "    (\"piano\", \"pianos\"),\n",
    "    (\"halo\", \"halos\"),\n",
    "    (\"potato\", \"potatoes\"),\n",
    "    (\"tomato\", \"tomatoes\"),\n",
    "    (\"hero\", \"heroes\"),\n",
    "    (\"echo\", \"echoes\"),\n",
    "    (\"zero\", \"zeroes\"),\n",
    "    (\"kangaroo\", \"kangaroos\"),\n",
    "    (\"radio\", \"radios\"),\n",
    "    (\"studio\", \"studios\"),\n",
    "    (\"video\", \"videos\"),\n",
    "    (\"zoo\", \"zoos\"),\n",
    "    (\"bamboo\", \"bamboos\"),\n",
    "    (\"cargo\", \"cargoes\"),\n",
    "    (\"volcano\", \"volcanoes\"),\n",
    "    (\"tornado\", \"tornadoes\"),\n",
    "    (\"mosquito\", \"mosquitoes\"),\n",
    "    (\"buffalo\", \"buffaloes\"),\n",
    "    (\"domino\", \"dominoes\"),\n",
    "    (\"torpedo\", \"torpedoes\"),\n",
    "    (\"veto\", \"vetoes\"),\n",
    "    (\"alumnus\", \"alumni\"),\n",
    "    (\"alumna\", \"alumnae\"),\n",
    "    (\"medium\", \"media\"),\n",
    "    (\"memorandum\", \"memoranda\"),\n",
    "    (\"appendix\", \"appendices\"),\n",
    "    (\"index\", \"indices\"),\n",
    "    (\"matrix\", \"matrices\"),\n",
    "    (\"vertex\", \"vertices\"),\n",
    "    (\"axis\", \"axes\"),\n",
    "    (\"ox\", \"oxen\"),\n",
    "    (\"quiz\", \"quizzes\"),\n",
    "    (\"church\", \"churches\"),\n",
    "    (\"match\", \"matches\"),\n",
    "    (\"branch\", \"branches\"),\n",
    "    (\"peach\", \"peaches\"),\n",
    "    (\"lunch\", \"lunches\"),\n",
    "    (\"sandwich\", \"sandwiches\"),\n",
    "    (\"witch\", \"witches\"),\n",
    "    (\"pass\", \"passes\"),\n",
    "    (\"glass\", \"glasses\"),\n",
    "    (\"class\", \"classes\"),\n",
    "    (\"kiss\", \"kisses\"),\n",
    "    (\"bus\", \"buses\"),\n",
    "    (\"gas\", \"gases\"),\n",
    "    (\"status\", \"statuses\"),\n",
    "    (\"octopus\", \"octopuses\"),\n",
    "    (\"virus\", \"viruses\"),\n",
    "    (\"radius\", \"radii\"),\n",
    "    (\"genius\", \"geniuses\"),\n",
    "    (\"species\", \"species\"),\n",
    "    (\"series\", \"series\"),\n",
    "    (\"aircraft\", \"aircraft\"),\n",
    "    (\"means\", \"means\"),\n",
    "    (\"barracks\", \"barracks\"),\n",
    "    (\"salmon\", \"salmon\"),\n",
    "    (\"shrimp\", \"shrimp\"),\n",
    "    (\"trout\", \"trout\"),\n",
    "    (\"swine\", \"swine\"),\n",
    "    (\"hovercraft\", \"hovercraft\"),\n",
    "    (\"crossroads\", \"crossroads\"),\n",
    "    (\"headquarters\", \"headquarters\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('007', '007s'), ('0', '0s'), ('0-10-0', '0-10-0s'), ('0-10-2', '0-10-2s'), ('0-12-0', '0-12-0s')]\n"
     ]
    }
   ],
   "source": [
    "word_pairs = []\n",
    "with open('nouns.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(',')\n",
    "        if len(parts) >= 2:\n",
    "            singular = parts[0].strip()\n",
    "            plural = parts[1].strip()\n",
    "            word_pairs.append((singular, plural))\n",
    "            \n",
    "            # Check if composed of only lowercase letters and spaces\n",
    "            #if all(c.islower() or c.isspace() for c in singular) and all(c.islower() or c.isspace() for c in plural):\n",
    "print(word_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_vocab(word_pairs):\n",
    "    special_tokens = [\"<pad>\", \"<unk>\"]\n",
    "    tokens = set()\n",
    "    for s, p in word_pairs:\n",
    "        tokens.update(s)\n",
    "        tokens.update(p)\n",
    "        tokens.add(\"->\")\n",
    "        tokens.add(\";\")\n",
    "\n",
    "    token_list = special_tokens + sorted(tokens)\n",
    "    stoi = defaultdict(lambda: 1, {tok: i for i, tok in enumerate(token_list)})  # <unk> = 1\n",
    "    itos = {i: tok for tok, i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "def tokenize(text, stoi):\n",
    "    return [stoi[c] for c in text]\n",
    "\n",
    "def detokenize(indices, itos):\n",
    "    return ''.join(itos[i] for i in indices if i > 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(word_pairs, stoi, k=3, max_len=128):\n",
    "    dataset = []\n",
    "    for _ in range(1000):  # number of examples\n",
    "        context, target = create_context_examples(word_pairs, k=k)\n",
    "        input_ids = tokenize(context, stoi)\n",
    "        target_ids = tokenize(target, stoi)\n",
    "        \n",
    "        input_tensor = torch.tensor(input_ids + [0] * (max_len - len(input_ids)), dtype=torch.long)[:max_len]\n",
    "        target_tensor = torch.tensor(target_ids + [0] * (max_len - len(target_ids)), dtype=torch.long)[:max_len]\n",
    "        \n",
    "        dataset.append((input_tensor, target_tensor))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, word_pairs, stoi, itos, k=3, max_len=128):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 20\n",
    "    with torch.no_grad():\n",
    "        for _ in range(total):\n",
    "            context, target = create_context_examples(word_pairs, k=k)\n",
    "            input_ids = tokenize(context, stoi)\n",
    "            input_tensor = torch.tensor(input_ids + [0] * (max_len - len(input_ids)), dtype=torch.long)[:max_len].unsqueeze(0)\n",
    "\n",
    "            output_logits = model(input_tensor)\n",
    "            output_ids = output_logits.argmax(dim=-1)[0].tolist()\n",
    "\n",
    "            predicted = detokenize(output_ids[len(input_ids):], itos)\n",
    "            print(f\"{context}{predicted.strip()} (Expected: {target})\")\n",
    "            if predicted.strip().startswith(target):\n",
    "                correct += 1\n",
    "\n",
    "    print(f\"Accuracy: {correct}/{total}\")\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.build_vocab.<locals>.<lambda>()>,\n",
       "            {'<pad>': 0,\n",
       "             '<unk>': 1,\n",
       "             ' ': 2,\n",
       "             '!': 3,\n",
       "             '\"': 4,\n",
       "             '#': 5,\n",
       "             '$': 6,\n",
       "             '%': 7,\n",
       "             '&': 8,\n",
       "             \"'\": 9,\n",
       "             '(': 10,\n",
       "             ')': 11,\n",
       "             '*': 12,\n",
       "             '+': 13,\n",
       "             '-': 14,\n",
       "             '->': 15,\n",
       "             '.': 16,\n",
       "             '/': 17,\n",
       "             '0': 18,\n",
       "             '1': 19,\n",
       "             '2': 20,\n",
       "             '3': 21,\n",
       "             '4': 22,\n",
       "             '5': 23,\n",
       "             '6': 24,\n",
       "             '7': 25,\n",
       "             '8': 26,\n",
       "             '9': 27,\n",
       "             ';': 28,\n",
       "             '?': 29,\n",
       "             'A': 30,\n",
       "             'B': 31,\n",
       "             'C': 32,\n",
       "             'D': 33,\n",
       "             'E': 34,\n",
       "             'F': 35,\n",
       "             'G': 36,\n",
       "             'H': 37,\n",
       "             'I': 38,\n",
       "             'J': 39,\n",
       "             'K': 40,\n",
       "             'L': 41,\n",
       "             'M': 42,\n",
       "             'N': 43,\n",
       "             'O': 44,\n",
       "             'P': 45,\n",
       "             'Q': 46,\n",
       "             'R': 47,\n",
       "             'S': 48,\n",
       "             'T': 49,\n",
       "             'U': 50,\n",
       "             'V': 51,\n",
       "             'W': 52,\n",
       "             'X': 53,\n",
       "             'Y': 54,\n",
       "             'Z': 55,\n",
       "             '`': 56,\n",
       "             'a': 57,\n",
       "             'b': 58,\n",
       "             'c': 59,\n",
       "             'd': 60,\n",
       "             'e': 61,\n",
       "             'f': 62,\n",
       "             'g': 63,\n",
       "             'h': 64,\n",
       "             'i': 65,\n",
       "             'j': 66,\n",
       "             'k': 67,\n",
       "             'l': 68,\n",
       "             'm': 69,\n",
       "             'n': 70,\n",
       "             'o': 71,\n",
       "             'p': 72,\n",
       "             'q': 73,\n",
       "             'r': 74,\n",
       "             's': 75,\n",
       "             't': 76,\n",
       "             'u': 77,\n",
       "             'v': 78,\n",
       "             'w': 79,\n",
       "             'x': 80,\n",
       "             'y': 81,\n",
       "             'z': 82,\n",
       "             '²': 83,\n",
       "             'Å': 84,\n",
       "             'Æ': 85,\n",
       "             'É': 86,\n",
       "             'Ü': 87,\n",
       "             'à': 88,\n",
       "             'á': 89,\n",
       "             'â': 90,\n",
       "             'ã': 91,\n",
       "             'ä': 92,\n",
       "             'å': 93,\n",
       "             'æ': 94,\n",
       "             'ç': 95,\n",
       "             'è': 96,\n",
       "             'é': 97,\n",
       "             'ê': 98,\n",
       "             'ë': 99,\n",
       "             'í': 100,\n",
       "             'î': 101,\n",
       "             'ï': 102,\n",
       "             'ñ': 103,\n",
       "             'ó': 104,\n",
       "             'ô': 105,\n",
       "             'õ': 106,\n",
       "             'ö': 107,\n",
       "             'ø': 108,\n",
       "             'ú': 109,\n",
       "             'û': 110,\n",
       "             'ü': 111,\n",
       "             'ý': 112,\n",
       "             'þ': 113,\n",
       "             'ā': 114,\n",
       "             'ċ': 115,\n",
       "             'č': 116,\n",
       "             'ē': 117,\n",
       "             'ğ': 118,\n",
       "             'ī': 119,\n",
       "             'ō': 120,\n",
       "             'ő': 121,\n",
       "             'œ': 122,\n",
       "             'Ś': 123,\n",
       "             'ś': 124,\n",
       "             'š': 125,\n",
       "             'ū': 126,\n",
       "             'Ž': 127,\n",
       "             'ž': 128,\n",
       "             'ə': 129,\n",
       "             'ʻ': 130,\n",
       "             'α': 131,\n",
       "             'β': 132,\n",
       "             'γ': 133,\n",
       "             'δ': 134,\n",
       "             'κ': 135,\n",
       "             'λ': 136,\n",
       "             'μ': 137,\n",
       "             'ο': 138,\n",
       "             'σ': 139,\n",
       "             '฿': 140,\n",
       "             'ḍ': 141,\n",
       "             'ṇ': 142,\n",
       "             'ṣ': 143,\n",
       "             '–': 144,\n",
       "             '’': 145,\n",
       "             '№': 146,\n",
       "             'ℝ': 147,\n",
       "             '−': 148,\n",
       "             '♭': 149,\n",
       "             '♯': 150})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi, itos = build_vocab(word_pairs)\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thompalec/cs182/182Proj/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.0637\n",
      "Epoch 2, Loss: 9.5569\n",
      "Epoch 3, Loss: 9.0670\n",
      "Epoch 4, Loss: 8.5869\n",
      "Epoch 5, Loss: 8.1351\n",
      "Epoch 6, Loss: 7.7380\n",
      "Epoch 7, Loss: 7.4077\n",
      "Epoch 8, Loss: 7.1311\n",
      "Epoch 9, Loss: 6.9097\n",
      "Epoch 10, Loss: 6.7320\n",
      "Epoch 11, Loss: 6.5810\n",
      "Epoch 12, Loss: 6.4626\n",
      "Epoch 13, Loss: 6.3615\n",
      "Epoch 14, Loss: 6.2793\n",
      "Epoch 15, Loss: 6.2158\n",
      "Epoch 16, Loss: 6.1627\n",
      "Epoch 17, Loss: 6.1138\n",
      "Epoch 18, Loss: 6.0771\n",
      "Epoch 19, Loss: 6.0404\n",
      "Epoch 20, Loss: 6.0092\n",
      "earmark -> earmarks ; augmentation cystoplasty -> ssessessssssss (Expected: augmentation cystoplasties)\n",
      "syce -> syces ; not dog -> sssssssesessspsssssssssssessessssssss (Expected: not dogs)\n",
      "measurable space -> measurable spaces ; kilomegacycle -> sssssss (Expected: kilomegacycles)\n",
      "vanga -> vangas ; caching proxy -> sessspsssssssssssessessssssss (Expected: caching proxies)\n",
      "rekick -> rekicks ; catfall -> sssesessspsssssssssssessessssssss (Expected: catfalls)\n",
      "penalty phase -> penalty phases ; orbiton -> sssssssessessssssss (Expected: orbitons)\n",
      "vicinage -> vicinages ; buy-out -> sessspsssssssssssessessssssss (Expected: buy-outs)\n",
      "mantelet -> mantelets ; silanol -> sessspsssssssssssessessssssss (Expected: silanols)\n",
      "Sassanid -> Sassanids ; Goldilocks planet -> sssssssessessssssss (Expected: Goldilocks planets)\n",
      "Hamptonian -> Hamptonians ; raftsman -> psssssssssssessessssssss (Expected: raftsmen)\n",
      "leod -> leod ; hoody -> ssssssssssesessspsssssssssssessessssssss (Expected: hoodies)\n",
      "clickstream -> clickstreams ; webphone -> ssssssssssessessssssss (Expected: webphones)\n",
      "membrane potential -> membrane potentials ; finishing-school ->  (Expected: finishing-schools)\n",
      "starfucker -> starfuckers ; polycentropodid -> sssssessessssssss (Expected: polycentropodids)\n",
      "fluate -> fluates ; tschaike -> ssesessspsssssssssssessessssssss (Expected: tschaikes)\n",
      "chyometer -> chyometers ; bivi -> esessspsssssssssssessessssssss (Expected: bivis)\n",
      "trillionth -> trillionths ; go-around -> sssssssssssessessssssss (Expected: go-arounds)\n",
      "superflux -> superfluxes ; pseudoprotocol -> sssssssessessssssss (Expected: pseudoprotocols)\n",
      "himantolophid -> himantolophids ; microcontroller -> ssessssssss (Expected: microcontrollers)\n",
      "exempt -> exempts ; millionnaire -> essspsssssssssssessessssssss (Expected: millionnaires)\n",
      "Accuracy: 0/20\n"
     ]
    }
   ],
   "source": [
    "stoi, itos = build_vocab(word_pairs)\n",
    "dataset = encode_dataset(word_pairs, stoi, k=1, max_len=64)\n",
    "model = InContextTransformer(vocab_size=len(stoi), d_model=128, nhead=8, num_layers=30)\n",
    "\n",
    "train_cycle(model, dataset, epochs=100, batch_size=1024)\n",
    "evaluate(model, word_pairs, stoi, itos, k=1, max_len=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
