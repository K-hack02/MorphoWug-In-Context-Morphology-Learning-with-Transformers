{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_examples(word_pairs, k=3):\n",
    "    \"\"\"\n",
    "    Formats data for in-context learning:\n",
    "    Input: \"sing1 -> plur1; sing2 -> plur2; ...; singN -> \"\n",
    "    Target: \"plurN\"\n",
    "    \"\"\"\n",
    "    context = []\n",
    "    for _ in range(k):\n",
    "        s, p = random.choice(word_pairs)\n",
    "        context.append(f\"{s} -> {p}\")\n",
    "    \n",
    "    test_singular, test_plural = random.choice(word_pairs)\n",
    "    context.append(f\"{test_singular} -> \")\n",
    "    \n",
    "    return \" ; \".join(context), test_plural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(dataset, batch_size):\n",
    "    \"\"\"Groups dataset items into batches of specified size\"\"\"\n",
    "    batch = []\n",
    "    for item in dataset:\n",
    "        batch.append(item)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:  # Yield remaining items\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InContextTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, nhead=2, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 512, d_model))\n",
    "        self.layers = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, 128),\n",
    "            num_layers\n",
    "        )\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) + self.pos_embed[:, :x.size(1)]\n",
    "        x = self.layers(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cycle(model, dataset, epochs=10, batch_size=32):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(dataset)\n",
    "        total_loss = 0\n",
    "        for batch in make_batches(dataset, batch_size):\n",
    "            inputs = torch.stack([ex[0] for ex in batch])\n",
    "            targets = torch.stack([ex[1] for ex in batch])\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    # Regular plurals\n",
    "    (\"cat\", \"cats\"),\n",
    "    (\"dog\", \"dogs\"),\n",
    "    (\"car\", \"cars\"),\n",
    "    (\"apple\", \"apples\"),\n",
    "    (\"book\", \"books\"),\n",
    "    (\"tree\", \"trees\"),\n",
    "    (\"cup\", \"cups\"),\n",
    "    (\"pen\", \"pens\"),\n",
    "    (\"chair\", \"chairs\"),\n",
    "    (\"table\", \"tables\"),\n",
    "    (\"house\", \"houses\"),\n",
    "    (\"phone\", \"phones\"),\n",
    "    (\"shoe\", \"shoes\"),\n",
    "    (\"bag\", \"bags\"),\n",
    "    (\"door\", \"doors\"),\n",
    "    (\"window\", \"windows\"),\n",
    "    (\"computer\", \"computers\"),\n",
    "    (\"student\", \"students\"),\n",
    "    (\"teacher\", \"teachers\"),\n",
    "    (\"doctor\", \"doctors\"),\n",
    "    (\"friend\", \"friends\"),\n",
    "    (\"child\", \"children\"),\n",
    "    (\"person\", \"people\"),\n",
    "    (\"man\", \"men\"),\n",
    "    (\"woman\", \"women\"),\n",
    "    (\"mouse\", \"mice\"),\n",
    "    (\"goose\", \"geese\"),\n",
    "    (\"tooth\", \"teeth\"),\n",
    "    (\"foot\", \"feet\"),\n",
    "    (\"fish\", \"fish\"),\n",
    "    (\"sheep\", \"sheep\"),\n",
    "    (\"deer\", \"deer\"),\n",
    "    (\"cactus\", \"cacti\"),\n",
    "    (\"focus\", \"foci\"),\n",
    "    (\"fungus\", \"fungi\"),\n",
    "    (\"nucleus\", \"nuclei\"),\n",
    "    (\"syllabus\", \"syllabi\"),\n",
    "    (\"analysis\", \"analyses\"),\n",
    "    (\"crisis\", \"crises\"),\n",
    "    (\"thesis\", \"theses\"),\n",
    "    (\"phenomenon\", \"phenomena\"),\n",
    "    (\"criterion\", \"criteria\"),\n",
    "    (\"datum\", \"data\"),\n",
    "    (\"bus\", \"buses\"),\n",
    "    (\"box\", \"boxes\"),\n",
    "    (\"fox\", \"foxes\"),\n",
    "    (\"watch\", \"watches\"),\n",
    "    (\"wish\", \"wishes\"),\n",
    "    (\"dish\", \"dishes\"),\n",
    "    (\"baby\", \"babies\"),\n",
    "    (\"city\", \"cities\"),\n",
    "    (\"party\", \"parties\"),\n",
    "    (\"story\", \"stories\"),\n",
    "    (\"berry\", \"berries\"),\n",
    "    (\"family\", \"families\"),\n",
    "    (\"country\", \"countries\"),\n",
    "    (\"lady\", \"ladies\"),\n",
    "    (\"boy\", \"boys\"),\n",
    "    (\"toy\", \"toys\"),\n",
    "    (\"key\", \"keys\"),\n",
    "    (\"day\", \"days\"),\n",
    "    (\"monkey\", \"monkeys\"),\n",
    "    (\"leaf\", \"leaves\"),\n",
    "    (\"wolf\", \"wolves\"),\n",
    "    (\"knife\", \"knives\"),\n",
    "    (\"life\", \"lives\"),\n",
    "    (\"wife\", \"wives\"),\n",
    "    (\"calf\", \"calves\"),\n",
    "    (\"half\", \"halves\"),\n",
    "    (\"loaf\", \"loaves\"),\n",
    "    (\"scarf\", \"scarves\"),\n",
    "    (\"chief\", \"chiefs\"),\n",
    "    (\"roof\", \"roofs\"),\n",
    "    (\"belief\", \"beliefs\"),\n",
    "    (\"chef\", \"chefs\"),\n",
    "    (\"photo\", \"photos\"),\n",
    "    (\"piano\", \"pianos\"),\n",
    "    (\"halo\", \"halos\"),\n",
    "    (\"potato\", \"potatoes\"),\n",
    "    (\"tomato\", \"tomatoes\"),\n",
    "    (\"hero\", \"heroes\"),\n",
    "    (\"echo\", \"echoes\"),\n",
    "    (\"zero\", \"zeroes\"),\n",
    "    (\"kangaroo\", \"kangaroos\"),\n",
    "    (\"radio\", \"radios\"),\n",
    "    (\"studio\", \"studios\"),\n",
    "    (\"video\", \"videos\"),\n",
    "    (\"zoo\", \"zoos\"),\n",
    "    (\"bamboo\", \"bamboos\"),\n",
    "    (\"cargo\", \"cargoes\"),\n",
    "    (\"volcano\", \"volcanoes\"),\n",
    "    (\"tornado\", \"tornadoes\"),\n",
    "    (\"mosquito\", \"mosquitoes\"),\n",
    "    (\"buffalo\", \"buffaloes\"),\n",
    "    (\"domino\", \"dominoes\"),\n",
    "    (\"torpedo\", \"torpedoes\"),\n",
    "    (\"veto\", \"vetoes\"),\n",
    "    (\"alumnus\", \"alumni\"),\n",
    "    (\"alumna\", \"alumnae\"),\n",
    "    (\"medium\", \"media\"),\n",
    "    (\"memorandum\", \"memoranda\"),\n",
    "    (\"appendix\", \"appendices\"),\n",
    "    (\"index\", \"indices\"),\n",
    "    (\"matrix\", \"matrices\"),\n",
    "    (\"vertex\", \"vertices\"),\n",
    "    (\"axis\", \"axes\"),\n",
    "    (\"ox\", \"oxen\"),\n",
    "    (\"quiz\", \"quizzes\"),\n",
    "    (\"church\", \"churches\"),\n",
    "    (\"match\", \"matches\"),\n",
    "    (\"branch\", \"branches\"),\n",
    "    (\"peach\", \"peaches\"),\n",
    "    (\"lunch\", \"lunches\"),\n",
    "    (\"sandwich\", \"sandwiches\"),\n",
    "    (\"witch\", \"witches\"),\n",
    "    (\"pass\", \"passes\"),\n",
    "    (\"glass\", \"glasses\"),\n",
    "    (\"class\", \"classes\"),\n",
    "    (\"kiss\", \"kisses\"),\n",
    "    (\"bus\", \"buses\"),\n",
    "    (\"gas\", \"gases\"),\n",
    "    (\"status\", \"statuses\"),\n",
    "    (\"octopus\", \"octopuses\"),\n",
    "    (\"virus\", \"viruses\"),\n",
    "    (\"radius\", \"radii\"),\n",
    "    (\"genius\", \"geniuses\"),\n",
    "    (\"species\", \"species\"),\n",
    "    (\"series\", \"series\"),\n",
    "    (\"aircraft\", \"aircraft\"),\n",
    "    (\"means\", \"means\"),\n",
    "    (\"barracks\", \"barracks\"),\n",
    "    (\"salmon\", \"salmon\"),\n",
    "    (\"shrimp\", \"shrimp\"),\n",
    "    (\"trout\", \"trout\"),\n",
    "    (\"swine\", \"swine\"),\n",
    "    (\"hovercraft\", \"hovercraft\"),\n",
    "    (\"crossroads\", \"crossroads\"),\n",
    "    (\"headquarters\", \"headquarters\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_vocab(word_pairs):\n",
    "    special_tokens = [\"<pad>\", \"<unk>\"]\n",
    "    tokens = set()\n",
    "    for s, p in word_pairs:\n",
    "        tokens.update(s)\n",
    "        tokens.update(p)\n",
    "        tokens.add(\"->\")\n",
    "        tokens.add(\";\")\n",
    "\n",
    "    token_list = special_tokens + sorted(tokens)\n",
    "    stoi = defaultdict(lambda: 1, {tok: i for i, tok in enumerate(token_list)})  # <unk> = 1\n",
    "    itos = {i: tok for tok, i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "def tokenize(text, stoi):\n",
    "    return [stoi[c] for c in text]\n",
    "\n",
    "def detokenize(indices, itos):\n",
    "    return ''.join(itos[i] for i in indices if i > 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(word_pairs, stoi, k=3, max_len=128):\n",
    "    dataset = []\n",
    "    for _ in range(1000):  # number of examples\n",
    "        context, target = create_context_examples(word_pairs, k=k)\n",
    "        input_ids = tokenize(context, stoi)\n",
    "        target_ids = tokenize(target, stoi)\n",
    "        \n",
    "        input_tensor = torch.tensor(input_ids + [0] * (max_len - len(input_ids)), dtype=torch.long)[:max_len]\n",
    "        target_tensor = torch.tensor(target_ids + [0] * (max_len - len(target_ids)), dtype=torch.long)[:max_len]\n",
    "        \n",
    "        dataset.append((input_tensor, target_tensor))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, word_pairs, stoi, itos, k=3, max_len=128):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 20\n",
    "    with torch.no_grad():\n",
    "        for _ in range(total):\n",
    "            context, target = create_context_examples(word_pairs, k=k)\n",
    "            input_ids = tokenize(context, stoi)\n",
    "            input_tensor = torch.tensor(input_ids + [0] * (max_len - len(input_ids)), dtype=torch.long)[:max_len].unsqueeze(0)\n",
    "\n",
    "            output_logits = model(input_tensor)\n",
    "            output_ids = output_logits.argmax(dim=-1)[0].tolist()\n",
    "\n",
    "            predicted = detokenize(output_ids[len(input_ids):], itos)\n",
    "            print(f\"{context}{predicted.strip()} (Expected: {target})\")\n",
    "            if predicted.strip().startswith(target):\n",
    "                correct += 1\n",
    "\n",
    "    print(f\"Accuracy: {correct}/{total}\")\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 173.5855\n",
      "Epoch 2, Loss: 160.7125\n",
      "Epoch 3, Loss: 158.8412\n",
      "Epoch 4, Loss: 158.3738\n",
      "Epoch 5, Loss: 157.8524\n",
      "Epoch 6, Loss: 157.2612\n",
      "Epoch 7, Loss: 156.6422\n",
      "Epoch 8, Loss: 156.4576\n",
      "Epoch 9, Loss: 155.6631\n",
      "Epoch 10, Loss: 156.1328\n",
      "crossroads -> crossroads ; city -> cities ; bamboo -> bamboos ; series ->  (Expected: series)\n",
      "glass -> glasses ; cat -> cats ; shrimp -> shrimp ; crisis -> sa (Expected: crises)\n",
      "chief -> chiefs ; genius -> geniuses ; halo -> halos ; index ->  (Expected: indices)\n",
      "syllabus -> syllabi ; car -> cars ; medium -> media ; toy -> ssa (Expected: toys)\n",
      "class -> classes ; hovercraft -> hovercraft ; bus -> buses ; window ->  (Expected: windows)\n",
      "appendix -> appendices ; criterion -> criteria ; syllabus -> syllabi ; datum ->  (Expected: data)\n",
      "photo -> photos ; torpedo -> torpedoes ; headquarters -> headquarters ; aircraft ->  (Expected: aircraft)\n",
      "studio -> studios ; scarf -> scarves ; shoe -> shoes ; friend ->  (Expected: friends)\n",
      "foot -> feet ; virus -> viruses ; party -> parties ; zero -> ssa (Expected: zeroes)\n",
      "datum -> data ; berry -> berries ; loaf -> loaves ; box -> ssssa (Expected: boxes)\n",
      "headquarters -> headquarters ; shoe -> shoes ; alumna -> alumnae ; index ->  (Expected: indices)\n",
      "alumna -> alumnae ; crossroads -> crossroads ; thesis -> theses ; apple ->  (Expected: apples)\n",
      "genius -> geniuses ; zoo -> zoos ; bus -> buses ; piano -> ssssa (Expected: pianos)\n",
      "toy -> toys ; memorandum -> memoranda ; apple -> apples ; halo ->  (Expected: halos)\n",
      "kiss -> kisses ; fox -> foxes ; alumnus -> alumni ; shoe -> sssa (Expected: shoes)\n",
      "thesis -> theses ; teacher -> teachers ; knife -> knives ; tornado ->  (Expected: tornadoes)\n",
      "foot -> feet ; aircraft -> aircraft ; child -> children ; crisis ->  (Expected: crises)\n",
      "man -> men ; analysis -> analyses ; headquarters -> headquarters ; cargo ->  (Expected: cargoes)\n",
      "status -> statuses ; cup -> cups ; branch -> branches ; knife ->  (Expected: knives)\n",
      "tree -> trees ; video -> videos ; bag -> bags ; table -> stssssa (Expected: tables)\n",
      "Accuracy: 0/20\n"
     ]
    }
   ],
   "source": [
    "stoi, itos = build_vocab(word_pairs)\n",
    "dataset = encode_dataset(word_pairs, stoi, k=3, max_len=64)\n",
    "model = InContextTransformer(vocab_size=len(stoi), d_model=64, nhead=2, num_layers=2)\n",
    "\n",
    "train_cycle(model, dataset, epochs=10, batch_size=16)\n",
    "evaluate(model, word_pairs, stoi, itos, k=3, max_len=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ff6a0c968a08784f678278e0ffcff5e26ba6040b0db9d7bd944b8e1a033ad1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
