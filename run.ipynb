{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamoosya/182Proj/blob/main/run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EaTXtm-Jf-tm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pS1ODlYioJv",
        "outputId": "f41bd992-9f4c-43ac-966e-7362ede7cc5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e0578592bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcVkXbUsoDK4"
      },
      "source": [
        "# Download modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9ZxqgsLlgBFy",
        "outputId": "bf2b9e85-4309-4928-e413-315a63381a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-04 07:50:31--  https://raw.githubusercontent.com/adamoosya/182Proj/main/hyperparameters.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3279 (3.2K) [text/plain]\n",
            "Failed to rename hyperparameters.py to hyperparameters.py.1: (2) No such file or directory\n",
            "Saving to: ‘hyperparameters.py’\n",
            "\n",
            "\rhyperparameters.py    0%[                    ]       0  --.-KB/s               \rhyperparameters.py  100%[===================>]   3.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-04 07:50:32 (62.9 MB/s) - ‘hyperparameters.py’ saved [3279/3279]\n",
            "\n",
            "--2025-05-04 07:50:32--  https://raw.githubusercontent.com/adamoosya/182Proj/main/tokenizer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815 [text/plain]\n",
            "Failed to rename tokenizer.py to tokenizer.py.1: (2) No such file or directory\n",
            "Saving to: ‘tokenizer.py’\n",
            "\n",
            "tokenizer.py        100%[===================>]     815  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-04 07:50:32 (51.6 MB/s) - ‘tokenizer.py’ saved [815/815]\n",
            "\n",
            "--2025-05-04 07:50:32--  https://raw.githubusercontent.com/adamoosya/182Proj/main/dataloader.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2055 (2.0K) [text/plain]\n",
            "Failed to rename dataloader.py to dataloader.py.1: (2) No such file or directory\n",
            "Saving to: ‘dataloader.py’\n",
            "\n",
            "dataloader.py       100%[===================>]   2.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-04 07:50:32 (56.4 MB/s) - ‘dataloader.py’ saved [2055/2055]\n",
            "\n",
            "--2025-05-04 07:50:32--  https://raw.githubusercontent.com/adamoosya/182Proj/main/model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6287 (6.1K) [text/plain]\n",
            "Failed to rename model.py to model.py.1: (2) No such file or directory\n",
            "Saving to: ‘model.py’\n",
            "\n",
            "model.py            100%[===================>]   6.14K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-04 07:50:32 (77.6 MB/s) - ‘model.py’ saved [6287/6287]\n",
            "\n",
            "--2025-05-04 07:50:32--  https://raw.githubusercontent.com/adamoosya/182Proj/main/data.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2153 (2.1K) [text/plain]\n",
            "Failed to rename data.py to data.py.1: (2) No such file or directory\n",
            "Saving to: ‘data.py’\n",
            "\n",
            "data.py             100%[===================>]   2.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-04 07:50:33 (57.9 MB/s) - ‘data.py’ saved [2153/2153]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('checkpoint'):\n",
        "    os.makedirs('checkpoint')\n",
        "\n",
        "url = f\"https://raw.githubusercontent.com/adamoosya/182Proj/main/hyperparameters.py\"\n",
        "!wget --no-cache --backups=1 {url}\n",
        "from hyperparameters import BATCH_SIZE, BLOCK_SIZE, MAX_ITERS, EVAL_INTERVAL, LEARNING_RATE, EVAL_ITERS, N_EMBD, N_HEAD, \\\n",
        "  N_LAYER, DROPOUT, DEVICE, DATA_CATEGORIES, TRAIN_DATA_CATEGORIES, VAL_DATA_CATEGORIES\n",
        "\n",
        "url = f\"https://raw.githubusercontent.com/adamoosya/182Proj/main/tokenizer.py\"\n",
        "!wget --no-cache --backups=1 {url}\n",
        "from tokenizer import tokenize, detokenize, IS_TO_TOKEN, AS_TOKEN, END_TOKEN, VOCAB_SIZE, CHAR_TO_TOKEN, TOKEN_TO_CHAR\n",
        "\n",
        "url = f\"https://raw.githubusercontent.com/adamoosya/182Proj/main/dataloader.py\"\n",
        "!wget --no-cache --backups=1 {url}\n",
        "from dataloader import get_context_test, get_context_example, get_batch\n",
        "\n",
        "url = f\"https://raw.githubusercontent.com/adamoosya/182Proj/main/model.py\"\n",
        "!wget --no-cache --backups=1 {url}\n",
        "from model import estimate_loss, GPTLanguageModel\n",
        "\n",
        "url = f\"https://raw.githubusercontent.com/adamoosya/182Proj/main/data.py\"\n",
        "!wget --no-cache --backups=1 {url}\n",
        "from data import load_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMpFyHslg124"
      },
      "source": [
        "# Get morphological transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CaXKOUZWg9Pb"
      },
      "outputs": [],
      "source": [
        "DATA, TRAIN_DATA, VAL_DATA = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIeYzs9mokUo"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "9P32nA1Tojjt"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataset, category=None, num_to_evaluate=100):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for _ in range(num_to_evaluate):\n",
        "        context, target = get_context_test(dataset, category=category)\n",
        "        if model.test(context, target):\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    model.train()\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dEXHU4YpHjs",
        "outputId": "dd2c08f7-89e2-40fd-ea3d-0098e9134e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.762783 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = GPTLanguageModel()\n",
        "model = model.to(DEVICE)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8AxyGwQp0I6",
        "outputId": "3d44f835-e865-4ae1-edf7-39174119f8be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 499, Train Score: 0.3800\n",
            "Epoch: 499, Validation Score: 0.2100\n",
            "Epoch: 999, Train Score: 0.3100\n",
            "Epoch: 999, Validation Score: 0.3000\n",
            "Epoch: 1499, Train Score: 0.5300\n",
            "Epoch: 1499, Validation Score: 0.3600\n",
            "Epoch: 1999, Train Score: 0.5500\n",
            "Epoch: 1999, Validation Score: 0.3400\n",
            "Epoch: 2499, Train Score: 0.4900\n",
            "Epoch: 2499, Validation Score: 0.3800\n",
            "Epoch: 2999, Train Score: 0.5300\n",
            "Epoch: 2999, Validation Score: 0.3000\n",
            "Epoch: 3499, Train Score: 0.5600\n",
            "Epoch: 3499, Validation Score: 0.3900\n",
            "Epoch: 3999, Train Score: 0.6500\n",
            "Epoch: 3999, Validation Score: 0.5100\n",
            "Epoch: 4499, Train Score: 0.5700\n",
            "Epoch: 4499, Validation Score: 0.5100\n",
            "Epoch: 4999, Train Score: 0.6400\n",
            "Epoch: 4999, Validation Score: 0.5000\n"
          ]
        }
      ],
      "source": [
        "for iter in range(MAX_ITERS):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % EVAL_INTERVAL == EVAL_INTERVAL-1:\n",
        "\n",
        "      train_score = evaluate(model, TRAIN_DATA, category=None)\n",
        "      print(f\"Epoch: {iter}, Train Score: {train_score:.4f}\")\n",
        "\n",
        "      val_score = evaluate(model, VAL_DATA, category=None)\n",
        "      print(f\"Epoch: {iter}, Validation Score: {val_score:.4f}\")\n",
        "\n",
        "      checkpoint_name = f\"epoch_{iter+1:04d}.pth\"  # Format with leading zeros\n",
        "      torch.save(model.state_dict(), os.path.join(\"checkpoint\", checkpoint_name))\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb, mask = get_batch(TRAIN_DATA)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb, mask)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkNyQuJDtzbT"
      },
      "source": [
        "# Statistics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses = {\n",
        "    'train': {category: [] for category in TRAIN_DATA_CATEGORIES},\n",
        "    'val': {category: [] for category in VAL_DATA_CATEGORIES}\n",
        "}\n",
        "accuracies = {\n",
        "    'train': {category: [] for category in TRAIN_DATA_CATEGORIES},\n",
        "    'val': {category: [] for category in VAL_DATA_CATEGORIES}\n",
        "}\n",
        "\n",
        "losses['train'][None] = []\n",
        "accuracies['train'][None] = []\n",
        "losses['val'][None] = []\n",
        "accuracies['val'][None] = []"
      ],
      "metadata": {
        "id": "TrMVYYhdbnSq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = \"checkpoint\"\n",
        "checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")]\n",
        "checkpoints.sort()\n",
        "\n",
        "for checkpoint_name in checkpoints:\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
        "    print(f\"Loading checkpoint: {checkpoint_name}\")\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    model.eval()\n",
        "\n",
        "    for split, dataset in zip(losses, [TRAIN_DATA, VAL_DATA]):\n",
        "        for category in losses[split]:\n",
        "            losses[split][category].append(estimate_loss(model, dataset, category))\n",
        "            accuracies[split][category].append(evaluate(model, dataset, category))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FPw-c44broM",
        "outputId": "697e8cda-49e5-443d-be14-a22c3e970199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint: epoch_0500.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Save the losses and accuracies dictionaries as some sort of file so that I can use them later, and show me how I can turn the file back into the losses and accuracies dictionaries after\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save the dictionaries to a file\n",
        "with open('losses_accuracies.pkl', 'wb') as f:\n",
        "    pickle.dump((losses, accuracies), f)\n",
        "\n",
        "# Load the dictionaries from the file\n",
        "with open('losses_accuracies.pkl', 'rb') as f:\n",
        "    loaded_losses, loaded_accuracies = pickle.load(f)\n",
        "\n",
        "# Verify that the loaded dictionaries are the same as the original ones\n",
        "print(losses == loaded_losses)\n",
        "print(accuracies == loaded_accuracies)\n"
      ],
      "metadata": {
        "id": "N_qLu45Ocp6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context, target = get_context_test(TRAIN_DATA, category=None)\n",
        "model.test(context, target, output=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14w6AYgENAjd",
        "outputId": "aeb95307-026a-4854-a62b-47cd16253d3a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: bluer#bluest$definer#, Prediction: definest, Expected: definest\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context, target = 'dog#dogs$cat#', 'cats'\n",
        "model.test(context, target, output=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNfS3VHvTbUT",
        "outputId": "4e5dcbeb-e26d-4d7f-dd07-9844f9384a87"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: dog#dogs$cat#, Prediction: cats, Expected: cats\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4L7m-JulZmTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want you to create a bunch of plots in matplotlib. I want to plot the train losses. The losses can be found in losses['train'] for each category. All the losses should be grouped up together in the same plot. The losses for category None should be 4x4 and on the left hand side. It should be labelled \"Train Loss\". There are 16 other categories. Those should all be put in a 4x4 grid next to the None category, and each should be 1x1. The x and y max and min values for all the plots should be standardized. The y value should be between 0 and 1, and the x value should be between 0 and 10.  The losses graph for the None category should be 4 times the size as the losses for the other categories\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'losses' dictionary is already populated as in your provided code.\n",
        "\n",
        "fig, axes = plt.subplots(4, 5, figsize=(20, 16))  # 4x5 grid for plots\n",
        "\n",
        "# Plot for category 'None' (larger size)\n",
        "axes[0, 0].plot(losses['train'][None])\n",
        "axes[0, 0].set_title(\"Train Loss (None)\")\n",
        "axes[0, 0].set_xlabel(\"Iteration\")\n",
        "axes[0, 0].set_ylabel(\"Loss\")\n",
        "axes[0, 0].set_xlim(0, 10)\n",
        "axes[0, 0].set_ylim(0, 1)\n",
        "\n",
        "# Merge the remaining subplots\n",
        "for i in range(4):\n",
        "    for j in range(1,5): # start from index 1\n",
        "        ax = axes[i][j]\n",
        "        category_index = i * 4 + j # Calculate the index to use for other categories\n",
        "\n",
        "        if category_index < len(TRAIN_DATA_CATEGORIES) and TRAIN_DATA_CATEGORIES[category_index] != None:\n",
        "          category = TRAIN_DATA_CATEGORIES[category_index]\n",
        "          ax.plot(losses['train'][category])\n",
        "          ax.set_title(f\"Train Loss ({category})\")\n",
        "          ax.set_xlabel(\"Iteration\")\n",
        "          ax.set_ylabel(\"Loss\")\n",
        "          ax.set_xlim(0, 10)\n",
        "          ax.set_ylim(0, 1)\n",
        "        else:\n",
        "          ax.axis('off') # Hide the empty subplots\n",
        "\n",
        "plt.savefig('train_losses.png')\n",
        "plt.tight_layout()  # Adjust spacing between subplots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kMW3r9JEPjO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(4, 5, figsize=(20, 16))  # 4x5 grid for plots\n",
        "\n",
        "# Plot for category 'None' (larger size)\n",
        "axes[0, 0].plot(accuracies['train'][None])\n",
        "axes[0, 0].set_title(\"Train Accuracy (None)\")\n",
        "axes[0, 0].set_xlabel(\"Iteration\")\n",
        "axes[0, 0].set_ylabel(\"Loss\")\n",
        "axes[0, 0].set_xlim(0, 10)\n",
        "axes[0, 0].set_ylim(0, 100)\n",
        "\n",
        "# Merge the remaining subplots\n",
        "for i in range(4):\n",
        "    for j in range(1,5): # start from index 1\n",
        "        ax = axes[i][j]\n",
        "        category_index = i * 4 + j # Calculate the index to use for other categories\n",
        "\n",
        "        if category_index < len(TRAIN_DATA_CATEGORIES) and TRAIN_DATA_CATEGORIES[category_index] != None:\n",
        "          category = TRAIN_DATA_CATEGORIES[category_index]\n",
        "          ax.plot(accuracies['train'][category])\n",
        "          ax.set_title(f\"Train Loss ({category})\")\n",
        "          ax.set_xlabel(\"Iteration\")\n",
        "          ax.set_ylabel(\"Loss\")\n",
        "          ax.set_xlim(0, 10)\n",
        "          ax.set_ylim(0, 100)\n",
        "        else:\n",
        "          ax.axis('off') # Hide the empty subplots\n",
        "\n",
        "plt.savefig('train_accuracy.png')\n",
        "plt.tight_layout()  # Adjust spacing between subplots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QdR99_jAVVKk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}